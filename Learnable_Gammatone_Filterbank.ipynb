{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learnable Gammatone Filterbank.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp4bQh3ugBqA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x  # code works on lower version of tensorflow"
      ],
      "metadata": {
        "id": "twxuUDzYgHvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "p2qi-ZENgHzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "metadata": {
        "id": "6B6qBJw4gH2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/LGFB/lgtfb-en"
      ],
      "metadata": {
        "id": "sMt7oJwpgH4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipdb"
      ],
      "metadata": {
        "id": "jWYTVmG_gH7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sys import argv\n",
        "import ipdb\n",
        "import matplotlib\n",
        "#matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8sjcNCTTgrff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline  \n",
        "import gc\n",
        "import pickle\n",
        "import random\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras import optimizers, losses, activations, models\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
        "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
        "    concatenate\n",
        "from numpy import random\n",
        "import librosa\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random"
      ],
      "metadata": {
        "id": "h8Cw-S2bgH-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline  \n",
        "import gc\n",
        "import pickle\n",
        "import random\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras import optimizers, losses, activations, models\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
        "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
        "    concatenate\n",
        "from numpy import random\n",
        "import librosa\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random"
      ],
      "metadata": {
        "id": "GTTxOSpGiHqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data resampling and storing signals in a way model accepts"
      ],
      "metadata": {
        "id": "6cogaiWqhQLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get all signals in a list\n",
        "import os  \n",
        "import librosa\n",
        "import librosa.display\n",
        "import pylab \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import struct\n",
        "from scipy.io import wavfile as wav\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd                                                                                                          \n",
        "dir =  \"/content/drive/MyDrive/DATA\" \n",
        "paths = []  \n",
        "y = []                                                                                                           \n",
        "for root, subdirectories, files in os.walk(dir):\n",
        "    for subdirectory in subdirectories:\n",
        "        f = os.path.join(root, subdirectory)\n",
        "        x = []\n",
        "        for filename in os.listdir(f):\n",
        "          #print(filename)\n",
        "          t,ex = os.path.splitext(filename)\n",
        "          if ex == \".wav\":\n",
        "            x.append(os.path.join(f,filename))\n",
        "            #c = os.path.dirname(paths[-1])\n",
        "            #c = os.path.basename(c)\n",
        "            #print(c)\n",
        "            #y.append(labels.get(c))\n",
        "          else:\n",
        "            continue\n",
        "        paths.append(x)\n",
        "print(len(paths))"
      ],
      "metadata": {
        "id": "Y_RRJEingIBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to check the response of a single signal from each class, make a list of the inputs\n",
        "single = [paths[0][190],paths[1][190],paths[2][190],paths[3][190],paths[4][190]]\n",
        "single"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KygkW0g9N1nE",
        "outputId": "d216790c-d1cd-4e85-d60f-8b8e9de21a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/DATA/MVP_New/New_MVP_194.wav',\n",
              " '/content/drive/MyDrive/DATA/MR_New/New_MR_191.wav',\n",
              " '/content/drive/MyDrive/DATA/N_New/New_N_175.wav',\n",
              " '/content/drive/MyDrive/DATA/AS_New/New_AS_184.wav',\n",
              " '/content/drive/MyDrive/DATA/MS_New/New_MS_193.wav']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting into subsets that contain equal and same distribution of classes. make it 2 subsets in case of train/validation splits"
      ],
      "metadata": {
        "id": "c9j40P5wiAA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for renaming the signal from original dataset and storing in the respective subset\n",
        "# renaming name in the subste num_class label_class name_id(random number to differentiate signals from class)\n",
        "import shutil\n",
        "src = \"/content/drive/MyDrive/DATA\"\n",
        "dest = \"/content/drive/MyDrive/LGFB/lgtfb-en/Data/valvular\"\n",
        "labels = {'MR_New':0,'MVP_New':1,'AS_New':2,'N_New':3,'MS_New':4} \n",
        "def save_fold(files,fold,y):\n",
        "  #c = os.path.dirname(files[0])\n",
        "  cnt = 1\n",
        "  for file in files:\n",
        "    print(file)\n",
        "    c = os.path.dirname(file)\n",
        "    c = os.path.basename(c)\n",
        "    label = labels.get(c)\n",
        "    y.append(c)\n",
        "    dest_path = os.path.join(dest, str(fold+1) + str('-') + str(cnt) + str('-') + str(c) + str('-') + str(label) + '.wav')\n",
        "    cnt = cnt+1\n",
        "    shutil.copy(file, dest_path)"
      ],
      "metadata": {
        "id": "zSKBzxzsohJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import numpy as np\n",
        "import random\n",
        "files = paths\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "for fold in range(2):  # change range for 5 if you need more subsets, 2 is for train/test split\n",
        "  print(fold)\n",
        "  for i in range(len(files)):\n",
        "    x = files[i]\n",
        "    print(len(files[i]))\n",
        "    if fold != 1:  # fold!=number of folds-1 ; last subset contains all the remaining signals\n",
        "      random_files = random.sample(x, 120)  # number of signals from each class in a subset \n",
        "      save_fold(random_files,fold,y_train)\n",
        "      x_train = x_train + random_files\n",
        "    else:\n",
        "      random_files = x\n",
        "      save_fold(random_files,fold,y_test)\n",
        "      x_test = x_test + random_files\n",
        "    #print(random_files)\n",
        "    C  = list(set(files[i]) - set(random_files)) + list(set(random_files) - set(files[i]))\n",
        "    print(len(C))\n",
        "    files[i] = C\n",
        "    print(len(files[i]))\n",
        "    print(\"\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XABK_Fi1okc-",
        "outputId": "83865b33-3bc2-4bca-8e00-c9a514f0385e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "200\n",
            "80\n",
            "80\n",
            "\n",
            "\n",
            "200\n",
            "80\n",
            "80\n",
            "\n",
            "\n",
            "200\n",
            "80\n",
            "80\n",
            "\n",
            "\n",
            "200\n",
            "80\n",
            "80\n",
            "\n",
            "\n",
            "200\n",
            "80\n",
            "80\n",
            "\n",
            "\n",
            "1\n",
            "80\n",
            "0\n",
            "0\n",
            "\n",
            "\n",
            "80\n",
            "0\n",
            "0\n",
            "\n",
            "\n",
            "80\n",
            "0\n",
            "0\n",
            "\n",
            "\n",
            "80\n",
            "0\n",
            "0\n",
            "\n",
            "\n",
            "80\n",
            "0\n",
            "0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for single signal from each class"
      ],
      "metadata": {
        "id": "WYUWVTlQjN3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for storing the single signals from each class in appropriate format\n",
        "# function for renaming the signal from original dataset and storing in the respective subset\n",
        "import shutil\n",
        "src = \"/content/drive/MyDrive/DATA\"\n",
        "dest = \"/content/drive/MyDrive/LGFB/lgtfb-en/Data/valvular1\"\n",
        "labels = {'MR_New':0,'MVP_New':1,'AS_New':2,'N_New':3,'MS_New':4} \n",
        "def save_fold(files,fold,y):\n",
        "  #c = os.path.dirname(files[0])\n",
        "  cnt = 1\n",
        "  for file in files:\n",
        "    print(file)\n",
        "    c = os.path.dirname(file)\n",
        "    c = os.path.basename(c)\n",
        "    label = labels.get(c)\n",
        "    y.append(c)\n",
        "    dest_path = os.path.join(dest, str(fold+1) + str('-') + str(cnt) + str('-') + str(c) + str('-') + str(label) + '.wav')\n",
        "    cnt = cnt+1\n",
        "    shutil.copy(file, dest_path)"
      ],
      "metadata": {
        "id": "dw0S9uEThFf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_fold(single,2,y_train)"
      ],
      "metadata": {
        "id": "O5xy8bFHgID2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation. stores input signals in tfr records so that they can processed any time even after session ends. This way the split remains same"
      ],
      "metadata": {
        "id": "DnQj1KF4jZSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to handle variable length signals that pads with silence for shorter durations\n",
        "input_length = 8000  # number of input samples you are going to send to model\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def audio_norm(data):\n",
        "\n",
        "    max_data = np.max(data)\n",
        "    min_data = np.min(data)\n",
        "    data = (data-min_data)/(max_data-min_data+0.0001)\n",
        "    return data-0.5\n",
        "\n",
        "\n",
        "def load_audio_file(file_path, input_length=input_length):\n",
        "    data = librosa.core.load(file_path, sr=2000)[0] #, sr=16000\n",
        "    if len(data)>input_length:\n",
        "        \n",
        "        \n",
        "        max_offset = len(data)-input_length\n",
        "        \n",
        "        offset = np.random.randint(max_offset)\n",
        "        \n",
        "        data = data[offset:(input_length+offset)]\n",
        "        \n",
        "        \n",
        "    else:\n",
        "        \n",
        "        max_offset = input_length - len(data)\n",
        "        print(max_offset)\n",
        "        \n",
        "        offset = np.random.randint(max_offset)\n",
        "        \n",
        "        \n",
        "        data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
        "        \n",
        "        \n",
        "    data = audio_norm(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "u1JHajoSgIGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function stores the subset split signals in tfr records\n",
        "import os\n",
        "import ipdb\n",
        "from audioread import NoBackendError\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "\n",
        "def bytes_feature(value):\n",
        "  '''\n",
        "  Creates a TensorFlow Record Feature with value as a byte array.\n",
        "  '''\n",
        "\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def int64_feature(value):\n",
        "  '''\n",
        "  Creates a TensorFlow Record Feature with value as a 64 bit integer.\n",
        "  '''\n",
        "\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def prepare_tfrecord_raw(example_paths, destination_path):\n",
        "  # Open a TFRecords file for writing.\n",
        "  writer = tf.io.TFRecordWriter(destination_path)\n",
        "  for idx in range(len(example_paths)):\n",
        "    # Load an audio file for preprocessing.\n",
        "    print('Extracting %s' % example_paths[idx])\n",
        "    try:\n",
        "      samples = load_audio_file(example_paths[idx],8000)  # input length\n",
        "    except NoBackendError:\n",
        "      print('Warning: Could not load {}.'.format(example_paths[idx]))\n",
        "      continue\n",
        "\n",
        "    # parsing filename\n",
        "    wav_id   = os.path.split(example_paths[idx])[-1].split('-')[1]\n",
        "    label    = int(os.path.split(example_paths[idx])[-1].split('.')[0].split('-')[-1].split('_')[0])\n",
        "\n",
        "    wav_id = bytes(wav_id, 'utf-8')\n",
        "\n",
        "    example = tf.train.Example(features=tf.train.Features(feature={\n",
        "      'wavform': bytes_feature(samples.flatten().tostring()),\n",
        "      'label': int64_feature(label),\n",
        "      'wav_id': bytes_feature(wav_id)\n",
        "    }))\n",
        "    writer.write(example.SerializeToString())\n",
        "  writer.close()\n",
        "\n",
        "\n",
        "# original dataset\n",
        "SOUND_FILE_DIR = 'Data/valvular'\n",
        "for fold in range(2):   # no of subsets\n",
        "  wav_paths = [os.path.join(SOUND_FILE_DIR, f) for f in os.listdir(SOUND_FILE_DIR) if f.endswith('.wav') and f.startswith(str(fold+1))]\n",
        "  print(len(wav_paths))\n",
        "  print(\" \")\n",
        "  dest_path = os.path.join('Data/raw_esc_' + str(fold+1) +'.tfrecords')\n",
        "  prepare_tfrecord_raw(wav_paths, dest_path)"
      ],
      "metadata": {
        "id": "nvtWi_zWgIJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL"
      ],
      "metadata": {
        "id": "aGV7eWLfk5I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "# hyper parameters \n",
        "BATCH_SIZE = 20\n",
        "EPOCHS = 200 #375#350#400 #250 \n",
        "LEARNING_RATE = 1e-2\n",
        "LR_DECAY_BASE = 1.02\n",
        "#LEARNING_RATE = 1e-4\n",
        "#LR_DECAY_BASE = 1.00\n",
        "weight_decay = 1e-3\n",
        "ALPHA = 1.0\t\t# mixup alpha\n",
        "N_CLASSES = 5\n",
        "DKP = 0.25\t\t# keep prob for dropout\n",
        "EMA_DECAY = 0.90\n",
        "\n",
        "trial   = 0\n",
        "CV_IDX  = 0 # [0,... 4]  it traverses the subsets\n",
        "\n",
        "tbpath = '/content/drive/MyDrive/LGFB2/lgtfb-en/Tensorboard/lgtfb_' + str(CV_IDX) + '_' + str(trial)\n",
        "mpath = '/content/drive/MyDrive/LGFB2/lgtfb-en/Model/lgtfb_' + str(CV_IDX) + '_' + str(trial)  # stores the best model\n",
        "\n",
        "tf.io.gfile.makedirs(tbpath)\n",
        "tf.io.gfile.makedirs(mpath)"
      ],
      "metadata": {
        "id": "YM2EFoi2gIL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_JS3(X, is_training=False):\n",
        "  with tf.name_scope('CNN'):\n",
        "    # 1st convolutional layer\n",
        "    h_conv1 = tf.layers.conv2d(X, 32, [5, 5], [1, 1], 'same', use_bias=False)\n",
        "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 4, 2, 1], strides=[1, 4, 2, 1], padding='SAME') \n",
        "    h_bn1   = tf.layers.batch_normalization(h_pool1, training=is_training)\n",
        "    h_1     = tf.nn.relu(h_bn1)\n",
        "    # 2nd convolutional layer\n",
        "    h_conv2 = tf.layers.conv2d(h_1, 64, [5, 5], [1, 1], 'same', use_bias=False)\n",
        "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 4, 2, 1], strides=[1, 4, 2, 1], padding='SAME')\n",
        "    h_bn2   = tf.layers.batch_normalization(h_pool2, training=is_training)\n",
        "    h_2     = tf.nn.relu(h_bn2)\n",
        "    # 3nd convolutional layer\n",
        "    h_conv3 = tf.layers.conv2d(h_2, 128, [5, 5], [1, 1], 'same', use_bias=False)\n",
        "    h_pool3 = tf.nn.max_pool(h_conv3, ksize=[1, 4, 2, 1], strides=[1, 4, 2, 1], padding='SAME')\n",
        "    h_bn3   = tf.layers.batch_normalization(h_pool3, training=is_training)\n",
        "    h_3     = tf.nn.relu(h_bn3)\n",
        "    return h_1, h_2, h_3"
      ],
      "metadata": {
        "id": "6n11yxQngIOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temporal average pooling and flatteing 2\n",
        "def TAP_FLAT(X, kp = 1.0, RC = 32, is_training=False):\n",
        "  tsX   = tf.shape(X)                                                   # [B x T x F x C]\n",
        "  slX   = X.get_shape().as_list()\n",
        "  Xtap  = tf.reduce_mean(X, 1)                                          # [B x F x C]\n",
        "  Xr    = tf.reshape(Xtap, [tsX[0], slX[2]*slX[3]])                     # [B x FC]\n",
        "  Xrd   = tf.nn.dropout(Xr, keep_prob = kp)                             # [B x FC]\n",
        "  M     = tf.layers.dense(Xrd, RC, use_bias=False)                      # [B x RC]\n",
        "  M     = tf.layers.batch_normalization(M, training=is_training)\n",
        "  M     = tf.nn.relu(M)                                                 # [B x RC]\n",
        "  return M\n",
        "\n",
        "def get_getter(ema):\n",
        "  def ema_getter(getter, name, *args, **kwargs):\n",
        "    var = getter(name, *args, **kwargs)\n",
        "    ema_var = ema.average(var)\n",
        "    return ema_var if ema_var else var\n",
        "  return ema_getter"
      ],
      "metadata": {
        "id": "7BNUODXrgIRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_filter_norm(filt):\n",
        "  return tf.sqrt(tf.reduce_sum(filt*filt,[0,1,2],keep_dims=True)+1e-4)\n",
        "\n",
        "# initialize center frequencies by using mel-scale\n",
        "def init_cent_freq(num_chan, max_f = 2000.0): #22050.0\n",
        "  mel_f  = np.linspace(0, 1127*np.log(1+max_f/700), num_chan+1, True)   # num_chan+1 points from 0 to max_mel\n",
        "  ori_f  = 700 * (np.exp(mel_f / 1127) - 1)\n",
        "  print(\"ori_f\",ori_f)\n",
        "  print(\"max_f\",max_f)\n",
        "  return ori_f\n",
        "\n",
        "\n",
        "def freq2tf(freq, nBins=128, level=7):\n",
        "  freq   = freq / freq[-1]                         # in [0~1]\n",
        "  centBin = int(nBins/2)\n",
        "  tree_freq = freq[centBin:centBin+1]\n",
        "  for lev in range(1,level):\n",
        "    for n in range(2**lev):\n",
        "      step = int(nBins / (2**lev))\n",
        "      st   = step*n\n",
        "      ed   = step*(n+1)\n",
        "      cr = int((st+ed)/2)\n",
        "      tree_freq = np.concatenate((tree_freq, [(freq[cr]-freq[st])/(freq[ed]-freq[st])]))\n",
        "  return tree_freq\n",
        "\n",
        "def tf2freq(tree_freq, level=7, LS=3):\n",
        "  freq = tf.zeros(shape=[1],dtype=tf.float32)\n",
        "  for lev in range(level):\n",
        "    N = freq.get_shape().as_list()[0]\n",
        "    st = 2**lev-1\n",
        "    ed = st*2 + 1\n",
        "    freq = tf.reshape(tf.stack([freq,freq],1),shape=[N*2])\n",
        "    split = tf.reshape(tf.stack([tf.log(tree_freq[st:ed]+1e-8),tf.log(1-tree_freq[st:ed]+1e-8)],1),shape=[N*2])\n",
        "    freq = freq + split\n",
        "  N     = freq.get_shape().as_list()[0]\n",
        "  freq  = tf.stack([freq]*LS,1)\n",
        "  split = tf.log(tf.constant([[1.0/LS]*LS],dtype=tf.float32))\n",
        "  freq  = tf.reshape(freq + split, shape=[N*LS])\n",
        "  nFreqDiff = tf.exp(freq)\n",
        "  print(\"nFreqDiff\",nFreqDiff)\n",
        "  freq = tf.cumsum(nFreqDiff)\n",
        "  print(\"freq\",freq)\n",
        "  maxfreq = freq[-1]\n",
        "  print(\"maxfreq\",maxfreq)\n",
        "  freq = freq / maxfreq\n",
        "  nFreqDiff = nFreqDiff / maxfreq\n",
        "  print(\"freq1\",freq)\n",
        "  return [freq, nFreqDiff]\n",
        "\n",
        "\n",
        "def inverse_sigmoid(sig_out):\n",
        "  return -np.log(1/sig_out -1 + 1e-8)"
      ],
      "metadata": {
        "id": "xPI9SdihgIw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LGTFB(X, kSize=2048, nBins=128, LS=3, nChan=3, is_training=False):\n",
        "  # new\n",
        " with tf.name_scope('LGTFB'):\n",
        "  init_freq      = init_cent_freq(nBins)  # initialize cfreq\n",
        "  level          = np.log2(nBins).astype(int)  # conversion of cfreq\n",
        "  tree_freq_init = freq2tf(init_freq, nBins, level) # conversion of cfreq\n",
        "  print(\"tree_freq_init\",tree_freq_init)\n",
        "  tree_freq_init = inverse_sigmoid(tree_freq_init)  # conversion of cfreq\n",
        "  print(\"tree_freq_init inerse\",tree_freq_init)   # conversion of cfreq\n",
        "  tree_freq      = tf.nn.sigmoid(tf.get_variable('freq', [nBins-1], initializer=tf.constant_initializer(tree_freq_init)))\n",
        "\n",
        "  print(\"tree_freq\",tree_freq)\n",
        "  [freq,nFreqDiff]  = tf2freq(tree_freq, level, LS)                \n",
        "  temp = tf.convert_to_tensor(init_freq, np.float32)\n",
        "  freq           = tf.reshape(freq, [1,nBins*LS,1])   # central freq  \n",
        "  print(\"cfreq\",freq.shape)   \n",
        "  nFreqDiff      = tf.reshape(nFreqDiff, [1,nBins*LS,1]) * 2  # band width\n",
        "  print(\"bwidth\",nFreqDiff.shape)\n",
        "  # gamma parameter\n",
        "  scale     = tf.nn.sigmoid(tf.get_variable('scale', [1,nBins*LS,nChan], initializer=tf.zeros_initializer() ))\n",
        "  print(\"scale\",scale.shape)\n",
        "  shape     = tf.exp(tf.get_variable('shape', [1,nBins*LS,nChan], initializer=tf.zeros_initializer())) # order\n",
        "  print(\"shape\",shape.shape)\n",
        "  # make filters\n",
        "  n      = tf.cumsum(tf.ones(shape=[kSize,nBins*LS,1],dtype=tf.float32),0)\t# [2048x128x1] \n",
        "  print(\"n\",n.shape)\n",
        "  gamma_1  = tf.pow(n/kSize,shape-1)\n",
        "  print(\"gamma1\",gamma_1.shape)\n",
        "  gamma_2  = tf.exp(-np.pi*nFreqDiff*scale*n)\n",
        "  print(\"gamma2\",gamma_2.shape)\n",
        "  gamma  = gamma_1 * gamma_2\n",
        "  gamma  = gamma / tf.reduce_mean(gamma,0,keep_dims=True)\n",
        "  print(\"gamma\",gamma.shape)\n",
        "  tone   = tf.cos(np.pi*(freq*n))\t\t\t\n",
        "  kernel = gamma * tone\t\n",
        "  print(\"kernel1\",kernel.shape)\t\t\t\n",
        "  kernel = tf.reshape(kernel,[kSize,1,1,nBins*LS*nChan])\n",
        "  print(\"kernel2\",kernel.shape)\n",
        "  kernel /= _get_filter_norm(kernel)\n",
        "  # calc filter bank output\n",
        "  fbank  = tf.nn.conv2d(X, kernel, [1,32,1,1], padding='VALID')\n",
        "  print(\"fbank1\",fbank.shape)\n",
        "  fbank  = tf.log(tf.abs(fbank)+1)\n",
        "  print(\"fbank2\",fbank.shape)\n",
        "  tsX    = tf.shape(fbank)\n",
        "  print(\"tsX\",tsX)\n",
        "  fbank  = tf.reshape(fbank, [tsX[0],tsX[1],nBins*LS,nChan])\n",
        "  print(\"fbank3\",fbank.shape)\n",
        "  fbank  = tf.nn.max_pool(fbank, ksize=[1, 4, LS, 1], strides=[1, 4, LS, 1], padding='VALID') \n",
        "  print(\"fbank4\",fbank.shape)\n",
        "  return fbank,shape,freq,nFreqDiff,kernel,temp"
      ],
      "metadata": {
        "id": "tKLvC07RgIzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Model(X, drop_kp=1.0, is_training=False, scope='Model', reuse=None, getter=None):\n",
        "  with tf.variable_scope(scope, reuse=reuse, custom_getter=getter):\n",
        "    GTFB,shape,freq,nFreqDiff,kernel,init_freq= LGTFB(X, kSize=256,nBins=32, LS=2, nChan=1, is_training=is_training)\t\n",
        "    #GTFB = EN(GTFB,9) \n",
        "    c1, c2, c3 = CNN_JS3(GTFB, is_training)\n",
        "    h3 = TAP_FLAT(c3, drop_kp, 256, is_training)\n",
        "    h3 = tf.nn.dropout(h3, keep_prob=drop_kp)\n",
        "    logit = tf.layers.dense(h3, N_CLASSES)\n",
        "  return logit,GTFB,shape,freq,nFreqDiff,kernel,c1,init_freq"
      ],
      "metadata": {
        "id": "WKQsuMIogI2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_raw = [[\n",
        "    'Data/raw_esc_1.tfrecords'\n",
        "]]\n",
        "valid_set_raw = [['Data/raw_esc_2.tfrecords']]\n",
        "test_set_raw = [['Data/raw_esc_2.tfrecords']]\n",
        "test_sets = [['Data/raw_esc_2.tfrecords']]\n",
        "single_set_raw = [['Data/raw_esc_3.tfrecords']]"
      ],
      "metadata": {
        "id": "Hylg9hDklxDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decodes tfr records and reshapes 1D signals to (no of samplesx1x1) so that CNN can process this\n",
        "input_length = 8000\n",
        "\n",
        "batch_size = 20\n",
        "def _parse_function_train_raw(example_proto):\n",
        "  # parsing\n",
        "  features = {'wavform': tf.io.FixedLenFeature([], tf.string),\n",
        "              'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "\t      'wav_id': tf.io.FixedLenFeature([], tf.string)}\n",
        "  parsed_features = tf.io.parse_single_example(example_proto, features)\n",
        "\n",
        "  # decode waveform\n",
        "  waveform = tf.io.decode_raw(parsed_features['wavform'], tf.float32)\n",
        "  waveform = tf.reshape(waveform, [-1,1,1])\t\t# 1D sequence with variable length\n",
        "\n",
        "  # repeating\n",
        "  waveform = tf.cond(tf.less(tf.shape(waveform)[0], 8000), \n",
        "                     lambda: tf.tile(waveform, [tf.math.floordiv(20000,tf.shape(waveform)[0])+1, 1, 1]),\n",
        "                     lambda: waveform)\n",
        "\n",
        "  # cropping bootstrapping\n",
        "  #waveform = tf.image.random_crop(waveform, [6000,1,1]) \n",
        "\n",
        "  # label\n",
        "  label    = tf.cast(parsed_features['label'], tf.int64)\n",
        "\n",
        "  # waf_id\n",
        "  wav_id = parsed_features['wav_id']\n",
        "\n",
        "  return waveform, label, wav_id\n",
        "\n",
        "\n",
        "def _parse_function_eval_raw(example_proto):\n",
        "  # parsing\n",
        "  features = {'wavform': tf.io.FixedLenFeature([], tf.string),\n",
        "              'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "\t      'wav_id': tf.io.FixedLenFeature([], tf.string)}\n",
        "  parsed_features = tf.io.parse_single_example(example_proto, features)\n",
        "\n",
        "  # decode waveform\n",
        "  waveform = tf.io.decode_raw(parsed_features['wavform'], tf.float32)\n",
        "  waveform = tf.reshape(waveform, [-1,1,1])\t\t# 1D sequence with variable length\n",
        "\n",
        "  # repeating\n",
        "  waveform = tf.cond(tf.less(tf.shape(waveform)[0], 8000),\n",
        "                     lambda: tf.tile(waveform, [tf.math.floordiv(20000,tf.shape(waveform)[0])+1, 1, 1]),\n",
        "                     lambda: waveform)\n",
        "\n",
        "  # label\n",
        "  label    = tf.cast(parsed_features['label'], tf.int64)\n",
        "  # waf_id\n",
        "  wav_id = parsed_features['wav_id']\n",
        "\n",
        "  return waveform, label, wav_id\n",
        "\n",
        "def get_train_dataset_raw(file_paths, batch_size=20):\n",
        "  dataset = tf.data.TFRecordDataset(file_paths)\n",
        "  dataset = dataset.map(_parse_function_train_raw, num_parallel_calls=4)\n",
        "  dataset = dataset.shuffle(buffer_size=10000)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "def get_eval_dataset_raw(file_paths, batch_size=1):\n",
        "  dataset = tf.data.TFRecordDataset(file_paths)\n",
        "  dataset = dataset.map(_parse_function_eval_raw, num_parallel_calls=4)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "g8uYZ7vzlxHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build our dataflow graph.\n",
        "GRAPH = tf.Graph()\n",
        "with GRAPH.as_default():\n",
        "  ## placeholders\n",
        "  is_training = tf.compat.v1.placeholder(tf.bool)\n",
        "  drop_kp = tf.compat.v1.placeholder(tf.float32, shape=())\n",
        "\n",
        "  ## input processing\n",
        "  # datasets\n",
        "  dataset_train = get_train_dataset_raw(train_set_raw[CV_IDX], batch_size=BATCH_SIZE)\n",
        "  dataset_valid = get_eval_dataset_raw(valid_set_raw[CV_IDX])\n",
        "  dataset_test  = get_eval_dataset_raw(test_set_raw[CV_IDX])\n",
        "  # reinitializable iterator to get waveforms\n",
        "  iterator        = tf.compat.v1.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(dataset_train), (tf.TensorShape([None,None,1,1]), tf.TensorShape([None]), tf.TensorShape([None])))\n",
        "  iter_init_train = iterator.make_initializer(dataset_train)\n",
        "  iter_init_valid = iterator.make_initializer(dataset_valid)\n",
        "  iter_init_test  = iterator.make_initializer(dataset_test)\n",
        "  next_element = iterator.get_next()\n",
        "  WAVEFORMS  = next_element[0]\n",
        "  LABELS     = next_element[1]\n",
        "  WAV_ID     = next_element[2]\n",
        "  # waveform normalization\n",
        "  Xm, Xv = tf.nn.moments(WAVEFORMS, axes=[1], keepdims =True)\n",
        "  IX = tf.compat.v1.div((WAVEFORMS-Xm),tf.sqrt(Xv+1e-8))\n",
        "  # mix-up\n",
        "  LABELS_mix = tf.one_hot(LABELS, N_CLASSES, dtype=tf.float32)\t# [B x 10]\n",
        "  #[IX, LABELS_mix] = tf.cond(is_training, lambda: mixup(IX, LABELS_mix), lambda: [IX, LABELS_mix])\n",
        "\n",
        "  ## Model\n",
        "  pred_Y,GTFB,shape,freq,nFreqDiff,kernel,c1,init_freq = Model(IX, drop_kp, is_training)\n",
        "  ## EMA (exponential moving averaging)\n",
        "  ema_dec   = tf.compat.v1.placeholder(tf.float32, shape=())\n",
        "  ema       = tf.train.ExponentialMovingAverage(decay=ema_dec, zero_debias=True)\n",
        "  var_model = tf.get_collection('trainable_variables', 'Model')\n",
        "  EMA_OP    = ema.apply(var_model)\n",
        "  ema_Y,GTFB,shap,fre,nFreqDif,ker,c,init_fre = Model(IX, drop_kp, is_training, reuse=True, getter=get_getter(ema))\n",
        "\n",
        "  ## objective function\n",
        "  # sound classification loss\n",
        "  sc_loss = tf.reduce_mean(-1.0 * tf.reduce_sum(LABELS_mix*tf.log(tf.nn.softmax(pred_Y) + 1e-10),1) )\t# for mixup\n",
        "  tf.summary.scalar(\"sc_loss\", sc_loss)\n",
        "  # l2_loss\n",
        "  tvars = tf.trainable_variables()\n",
        "  l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tvars if 'dense' in v.name and 'kernel' in v.name])\n",
        "  tf.summary.scalar(\"l2_loss\", l2_loss)\n",
        "  # final cost\n",
        "  COST = sc_loss + weight_decay*l2_loss\n",
        "\n",
        "  ## calc num of tvars \n",
        "  nvars = 0\n",
        "  for var in tvars:\n",
        "    sh = var.get_shape().as_list()\n",
        "    print(var.name, sh)\n",
        "    nvars += np.prod(sh)\n",
        "  print(nvars, 'total variables')\n",
        "\n",
        "  ## computing gradients and optimization\n",
        "  lr = tf.compat.v1.placeholder(tf.float32, shape=())\n",
        "# OPTIMIZER = tf.train.AdamOptimizer(LEARNING_RATE)\t# default epsilon 1e-08\n",
        "  OPTIMIZER = tf.train.AdamOptimizer(lr)\t# default epsilon 1e-08\n",
        "  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "  with tf.control_dependencies(update_ops):\n",
        "    grads,_ = tf.clip_by_global_norm(tf.gradients(COST,tvars),1)\t# compute gradients and do clipping\n",
        "    APPLY_GRADIENT_OP = OPTIMIZER.apply_gradients(zip(grads, tvars))\t# apply gradients\n",
        "\n",
        "  # evaluation\n",
        "  correct_pred = tf.equal(tf.argmax(pred_Y, 1), tf.argmax(LABELS_mix,1))\t# for mixup accuracy\n",
        "  ACCURACY = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
        "  predicted = tf.argmax(pred_Y, 1)\n",
        "  actual = tf.argmax(LABELS_mix,1)\n",
        "  TP = tf.count_nonzero(predicted * actual)\n",
        "  TN = tf.count_nonzero((predicted - 1) * (actual - 1))\n",
        "  FP = tf.count_nonzero(predicted * (actual - 1))\n",
        "  FN = tf.count_nonzero((predicted - 1) * actual)\n",
        "  # confusion matrix\n",
        "  CONF_MAT = tf.confusion_matrix(LABELS, tf.argmax(pred_Y, 1), num_classes=N_CLASSES)\n",
        "  # ema evaluation for EMA\n",
        "  ema_correct_pred = tf.equal(tf.argmax(ema_Y, 1), tf.argmax(LABELS_mix,1))\t# for mixup accuracy\n",
        "  EMA_ACCURACY = tf.reduce_mean(tf.cast(ema_correct_pred, dtype=tf.float32))\n",
        "  # confusion matrix for EMA\n",
        "  EMA_CONF_MAT = tf.confusion_matrix(LABELS, tf.argmax(ema_Y, 1), num_classes=N_CLASSES)\n",
        " \n",
        "  SUMMARIES_OP = tf.summary.merge_all()\n",
        "\n",
        "\n",
        "# Start training the model.\n",
        "with tf.Session(graph=GRAPH) as SESSION:\n",
        "  # initialize first\n",
        "  SESSION.run(tf.global_variables_initializer())\n",
        "  # Create a tensorflow summary writer.\n",
        "  SUMMARY_WRITER = tf.summary.FileWriter(tbpath, graph=GRAPH)\n",
        "  # Create a tensorflow graph writer.\n",
        "  GRAPH_WRITER = tf.train.Saver(max_to_keep=1)\n",
        "\n",
        "  steps = 0     # for tf.summary stepB\n",
        "  best_cost = float('Inf')\n",
        "  best_acc  = float(0)\n",
        "  best_model = 0\n",
        "\n",
        "  train_acc_hist  = []\n",
        "  valid_acc_hist  = []\n",
        "  test_acc_hist   = []\n",
        "  ema_valid_acc_hist  = []\n",
        "  ema_test_acc_hist   = []\n",
        "  train_cost_hist = []\n",
        "  test_cost_hist   = []\n",
        "  ord_hist = []\n",
        "  cfreq_hist = []\n",
        "  bwidth_hist = []\n",
        "  init_cfreq = []\n",
        "\n",
        "  for EPOCH in range(EPOCHS):\n",
        "    # initialize an iterator over the training dataset.\n",
        "    SESSION.run(iter_init_train)\n",
        "    iters = 0\n",
        "    costs = 0.0\n",
        "    accs  = 0.0\n",
        "\n",
        "    lr_decay = LR_DECAY_BASE ** (EPOCH)\n",
        "    lr_epoch = LEARNING_RATE / lr_decay\n",
        "    print('Epoch %d, Leanring rate = %.7f' % (EPOCH, lr_epoch))\n",
        "\n",
        "    start_time = time.time()\n",
        "    while True:\n",
        "      try:\n",
        "        _, summaries, COST_VAL, ACC_VAL = SESSION.run([APPLY_GRADIENT_OP, SUMMARIES_OP, COST, ACCURACY],\n",
        "                           feed_dict={drop_kp: DKP, is_training: True, lr: lr_epoch})\n",
        "        costs += COST_VAL\n",
        "        accs  += ACC_VAL\n",
        "        iters += 1\n",
        "        if iters % 20 == 0:\n",
        "          end_time = time.time()\n",
        "          DURATION = end_time - start_time\n",
        "          start_time = end_time\n",
        "          print('Epoch %d, Iters %d, cost = %.6f (%.3f sec)' % (EPOCH, iters, (costs/iters), DURATION))\n",
        "          SUMMARY_WRITER.add_summary(summaries, steps)\n",
        "          steps += 1\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        break\n",
        "    end_time = time.time()\n",
        "    DURATION = end_time - start_time\n",
        "    SUMMARY_WRITER.add_summary(summaries, steps)\n",
        "    steps += 1\n",
        "    print(iters)\n",
        "    print('Epoch %d, Train cost = %.6f, acc = %.6f (%.3f sec)' % (EPOCH, (costs/iters), (accs/iters),DURATION))\n",
        "    train_acc_hist.append(accs/iters)\n",
        "    train_cost_hist.append(costs/iters)\n",
        "\n",
        "    ### EMA ###\n",
        "    if EPOCH == 0:\n",
        "      ema_dec_tmp = 0\n",
        "    else:\n",
        "      ema_dec_tmp = EMA_DECAY\n",
        "    _ = SESSION.run([EMA_OP], feed_dict={ema_dec: ema_dec_tmp})\n",
        "\n",
        "    # Valid\n",
        "    SESSION.run(iter_init_valid)\n",
        "    iters = 0\n",
        "    accs  = 0.0\n",
        "    costs = 0.0\n",
        "    \n",
        "    while True:\n",
        "      try:\n",
        "        ACC_VAL, COST_VAL= SESSION.run([EMA_ACCURACY,COST], feed_dict={drop_kp: 1.0, is_training: False, lr: 0.0})\n",
        "        costs = costs + COST_VAL\n",
        "        accs  += ACC_VAL\n",
        "        iters += 1\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        break\n",
        "    final_acc  = (accs/iters)\n",
        "    print(iters)\n",
        "    print('(EMA) Epoch %d, Valid acc = %.6f' % (EPOCH,  final_acc))\n",
        "\n",
        "    ema_valid_acc_hist.append(final_acc)\n",
        "    test_cost_hist.append(costs/iters)\n",
        "\n",
        "    # update best model\n",
        "    if EPOCH > 19:\n",
        "      if final_acc > best_acc:\n",
        "        print('Best epoch is changed from %d to %d, Acc %.4f to %.4f' % (best_model, EPOCH, best_acc, final_acc))\n",
        "        best_acc = final_acc\n",
        "        best_model = EPOCH\n",
        "        GRAPH_WRITER.save(SESSION, mpath+'/model', global_step=EPOCH)   # save the best model\n",
        "\n",
        "    # Test\n",
        "    SESSION.run(iter_init_test)\n",
        "    iters = 0\n",
        "    accs  = 0.0\n",
        "    ord = 0.0\n",
        "    cfreq = 0.0\n",
        "    bwidth = 0.0\n",
        "    order = []\n",
        "    cfreq = []\n",
        "    bwidth = []\n",
        "    while True:\n",
        "      try:\n",
        "        ACC_VAL, ok,ck,bk,infreq = SESSION.run([EMA_ACCURACY,shape,freq,nFreqDiff,init_freq], feed_dict={drop_kp: 1.0, is_training: False, lr: 0.0})\n",
        "        accs  += ACC_VAL\n",
        "        if iters == 0:\n",
        "          order.append(ok)  # intermediate learnable parameter values over the training\n",
        "          cfreq.append(ck)\n",
        "          bwidth.append(bk)\n",
        "          init_cfreq.append(infreq)\n",
        "        iters += 1\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        break\n",
        "    final_acc  = (accs/iters)\n",
        "    print('(EMA) Epoch %d, Test acc = %.6f' % (EPOCH,  final_acc))\n",
        "    ema_test_acc_hist.append(final_acc)\n",
        "    ord_hist.append(order[0][0][40][0])   # to store one of the filters parameter values \n",
        "    cfreq_hist.append(cfreq[0][0][40][0])\n",
        "    bwidth_hist.append(bwidth[0][0][40][0])\n",
        "  epochs = range(1, len(train_acc_hist)+1)\n",
        "\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.plot(epochs, train_acc_hist, label='Training accuracy')\n",
        "  plt.plot(epochs, ema_valid_acc_hist,label='Validation accuracy')\n",
        "  #plt.plot(epochs, ema_test_acc_hist, label='Test accuracy')\n",
        "  plt.title('accuracy plot')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  #cost plot\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.plot(epochs, train_cost_hist, label='Training loss')\n",
        "  plt.plot(epochs, test_cost_hist,label='Validation loss')\n",
        "  #plt.plot(epochs, ema_test_acc_hist, label='Test accuracy')\n",
        "  plt.title('loss plot')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  #order plot\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.plot(epochs, ord_hist, label='order history')\n",
        "  plt.title('order hist')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('order')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  #cfreq plot\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.plot(epochs, cfreq_hist, label='central freq history')\n",
        "  plt.title('cfreq hist')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('cfreq')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  #bwidth plot\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.plot(epochs, bwidth_hist, label='bandwidth history')\n",
        "  plt.title('bwidth hist')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('bandwidth')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # test with the best model\n",
        "  GRAPH_WRITER.restore(SESSION, tf.train.latest_checkpoint(mpath))\n",
        "  print('Test using the best model %d in %s' % (best_model, mpath))\n",
        "  SESSION.run(iter_init_test)\n",
        "  iters = 0\n",
        "  accs = 0.0\n",
        "  conf_mat = np.zeros((2,2)).astype(int)\n",
        "  while True:\n",
        "    try:\n",
        "      [ACCS_VAL, CONF_MAT_VAL] = SESSION.run([EMA_ACCURACY, EMA_CONF_MAT], feed_dict={drop_kp: 1.0, is_training: False, lr: 0.0})\n",
        "      accs += ACCS_VAL\n",
        "      conf_mat = conf_mat + CONF_MAT_VAL\n",
        "      iters += 1\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      break\n",
        "  print('Test Accuracy : %.4f' % (accs/iters))\n",
        "  print(conf_mat)\n",
        "  print(test_sets)\n",
        "  dataset_test1  = get_eval_dataset_raw(test_sets)  # iterators for testing and single examples\n",
        "  iter_init_test1  = iterator.make_initializer(dataset_test1)\n",
        "  dataset_single  = get_eval_dataset_raw(single_set_raw)\n",
        "  iter_init_single  = iterator.make_initializer(dataset_single)"
      ],
      "metadata": {
        "id": "BpY-U4wewl4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coeff = []\n",
        "order = []\n",
        "cfreq = []\n",
        "bwidth = []"
      ],
      "metadata": {
        "id": "IXQVvK0hlxKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # test with the best model\n",
        "with tf.Session(graph=GRAPH) as SESSION:\n",
        "  GRAPH_WRITER.restore(SESSION, tf.train.latest_checkpoint(mpath))\n",
        "  print('Test using the best model %d in %s' % (best_model, mpath))\n",
        "  SESSION.run(iter_init_test)\n",
        "  iters = 0\n",
        "  accs = 0.0\n",
        "  conf_mat = np.zeros((5,5)).astype(int)\n",
        "  mat = np.zeros((5,5)).astype(int)\n",
        "  #pred = []\n",
        "  #true = []\n",
        "  Tp = 0.0\n",
        "  Tn = 0.0\n",
        "  Fp = 0.0\n",
        "  Fn = 0.0\n",
        "  layers = []   # to store the convolutional layer output just after LGTFB layer\n",
        "  filter_out = [] # to store the LGTFB out put\n",
        "  while True:\n",
        "    try:\n",
        "      #tf.argmax(pred_Y, 1), tf.argmax(LABELS_mix,1)\n",
        "      [ACCS_VAL, CONF_MAT_VAL,layer,conf,t,p,ok,ck,bk,weight,conv] = SESSION.run([EMA_ACCURACY, EMA_CONF_MAT,GTFB,CONF_MAT,LABELS_mix,pred_Y,shape,freq,nFreqDiff,kernel,c1], feed_dict={drop_kp: 1.0, is_training: False, lr: 0.0})\n",
        "      accs += ACCS_VAL\n",
        "      conf_mat = conf_mat + CONF_MAT_VAL\n",
        "      if iters == 0:\n",
        "        coeff.append(weight)\n",
        "        order.append(ok)\n",
        "        cfreq.append(ck)\n",
        "        bwidth.append(bk)\n",
        "      layers.append(conv)\n",
        "      filter_out.append(layer)\n",
        "      iters += 1\n",
        "      mat = mat + conf\n",
        "      print(t)\n",
        "      print(p)\n",
        "     # new_result = tf.argmax(t, 1).eval(session=tf.compat.v1.Session())\n",
        "      #print(new_result)\n",
        "      #pred = pred + new_result\n",
        "      #new_result = tf.argmax(p, 1).eval(session=tf.compat.v1.Session())\n",
        "      #true = true + new_result\n",
        "      #print(new_result)\n",
        "      #break\n",
        "      #pred.append(tf.argmax(p, 1).eval(session=tf.compat.v1.Session())[0])\n",
        "      #true.append(tf.argmax(t, 1).eval(session=tf.compat.v1.Session())[0])\n",
        "     # print(pred)\n",
        "      #print(true)\n",
        "      #break\n",
        "      #Tp = Tp+tp\n",
        "      #Tn = Tn + tn\n",
        "      #Fp = Fp + fp\n",
        "      #Fn = Fn + fn\n",
        "      #if iters%20 == 0:\n",
        "          #layers.append(layer)\n",
        "      #print(layer)\n",
        "      #output = layer\n",
        "      #break\n",
        "      #activations = np.array(activations).squeeze()\n",
        "      #print(activations)\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      break\n",
        "  print('Test Accuracy : %.4f' % (accs/iters))\n",
        "  print(conf_mat)\n",
        "  #m = tf.confusion_matrix(true, pred, num_classes=N_CLASSES)\n",
        "  #c = tf.compat.v1.metrics.recall(true,pred)\n",
        "  #Truey = true.eval(session=tf.compat.v1.Session())\n",
        "  #print(Truey)\n",
        "  #Pred = pred.eval(session=tf.compat.v1.Session())\n",
        "  #print(Pred)"
      ],
      "metadata": {
        "id": "4P_Qms9qlxNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification report\n",
        "x = conf_mat\n",
        "# Function for performance report.\n",
        "def performance_report(arr):\n",
        "    col = len(arr)\n",
        "    print(col)\n",
        "    # col=number of class\n",
        "  \n",
        "  \n",
        "    cr = dict()\n",
        "    support_sum = 0\n",
        "      \n",
        "    # macro avg of support is\n",
        "    # sum of support only, not the mean.\n",
        "    macro = [0]*3  \n",
        "      \n",
        "    # weighted avg of support is\n",
        "    # sum of support only, not the mean.\n",
        "    weighted = [0]*3\n",
        "    for i in range(col):\n",
        "        #vertical_sum = 0\n",
        "        #for j in range(col): \n",
        "          #print(\"J\")\n",
        "          #print(i)\n",
        "          #print(j)\n",
        "          #print(arr[j][i])\n",
        "          #vertical_sum= vertical_sum + arr[j][i] \n",
        "        vertical_sum= sum([arr[j][i] for j in range(col)])\n",
        "        horizontal_sum= sum(arr[i])\n",
        "        p = arr[i][i] / vertical_sum\n",
        "        r = arr[i][i] / horizontal_sum\n",
        "        f = (2 * p * r) / (p + r)\n",
        "        s = horizontal_sum\n",
        "        row=[p,r,f,s]\n",
        "        support_sum+=s\n",
        "        for j in range(3):\n",
        "            macro[j]+=row[j]\n",
        "            weighted[j]+=row[j]*s\n",
        "        cr[i]=row\n",
        "  \n",
        "    # add Accuracy parameters.\n",
        "    truepos=0\n",
        "    total=0\n",
        "    for i in range(col):\n",
        "        truepos+=arr[i][i]\n",
        "        total+=sum(arr[i])\n",
        "  \n",
        "    cr['Accuracy']=[\"\", \"\", truepos/total, support_sum]\n",
        "  \n",
        "    # Add macro-weight and weighted_avg features.\n",
        "    macro_avg=[Sum/col for Sum in macro]\n",
        "    macro_avg.append(support_sum)\n",
        "    cr['Macro_avg']=macro_avg\n",
        "  \n",
        "    weighted_avg=[Sum/support_sum for Sum in weighted]\n",
        "    weighted_avg.append(support_sum)\n",
        "    cr['Weighted_avg']=weighted_avg\n",
        "  \n",
        "    # print the classification_report\n",
        "    print(\"Performance report of the model is :\")\n",
        "    space,p,r,f,s=\" \",\"Precision\",\"Recall\",\"F1-Score\",\"Support\"\n",
        "    print(\"%13s %9s %9s %9s %9s\\n\"%(space,p,r,f,s))\n",
        "    stop=0\n",
        "    for key,value in cr.items():\n",
        "        if stop<col:\n",
        "            stop+=1\n",
        "            print(\"%13s %9.2f %9.2f %9.2f %9d\"%(key,value[0],\n",
        "                                                value[1],\n",
        "                                                value[2],\n",
        "                                                value[3]))\n",
        "        elif stop==col:\n",
        "            stop+=1\n",
        "            print(\"\\n%13s %9s %9s %9.2f %9d\"%(key,value[0],\n",
        "                                              value[1],\n",
        "                                              value[2],\n",
        "                                              value[3]))\n",
        "        else:\n",
        "            print(\"%13s %9.2f %9.2f %9.2f %9d\"%(key,\n",
        "                                                value[0],\n",
        "                                                value[1],\n",
        "                                                value[2],\n",
        "                                                value[3]))\n",
        "cr=performance_report(x)"
      ],
      "metadata": {
        "id": "aAfXy0gZlxQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot filter outputs\n",
        "import matplotlib.pyplot as plt \n",
        "for i in range(len(filter_out)):\n",
        "  temp = filter_out[i][0].transpose()\n",
        "  print(temp[0].shape)\n",
        "  print(len(temp[0]))\n",
        "  fig = plt.figure(figsize=(20, 20))\n",
        "  for j in range(len(temp[0])):\n",
        "    plt.subplot(8, 4, j+1)    \n",
        "    plt.plot(temp[0][j])\n",
        "  fig.tight_layout(pad=3.0)\n",
        "  plt.show()\n",
        "  print(\" \")\n",
        "  print(\" \")"
      ],
      "metadata": {
        "id": "m2XMC4GYlxSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation maps\n",
        "def plot_feature_maps(feature_maps):\n",
        "    height, width, depth = feature_maps.shape\n",
        "    nb_plot = int(np.rint(np.sqrt(depth)))\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    for i in range(depth):\n",
        "        #plt.subplot(8,4, i+1)\n",
        "        plt.imshow(feature_maps[:,:,i], cmap='viridis')\n",
        "        plt.title('feature map {}'.format(i+1))\n",
        "        fig.savefig('/content/drive/MyDrive/feature_maps/MS/feature map {}'.format(i+1))\n",
        "    fig.tight_layout(pad=3.0)\n",
        "    plt.show()\n",
        "for output in layers:\n",
        "  first_layer_activation = output\n",
        "  plot_feature_maps(first_layer_activation[0])\n",
        "  print(\" \")"
      ],
      "metadata": {
        "id": "dh44liaPscIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}